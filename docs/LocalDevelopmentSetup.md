# Local Development Setup Guide

This guide provides comprehensive instructions for setting up the Chat With Your Data Solution Accelerator for local development across Windows, Linux, and macOS platforms.

## Important Setup Notes

### Multi-Service Architecture

This application consists of **three separate services** that run independently:

1. **Frontend** - React-based user interface
2. **Admin Web** - Streamlit application for data management
3. **Batch Processing Function** - Azure Function for document processing

> **‚ö†Ô∏è Critical: Each service must run in its own terminal/console window**
>
> - **Do NOT close terminals** while services are running
> - Open **3 separate terminal windows** for local development
> - Each service will occupy its terminal and show live logs
>
> **Terminal Organization:**
> - **Terminal 1**: Frontend - Development server on port 5174
> - **Terminal 2**: Admin Web - Streamlit server on port 8501
> - **Terminal 3**: Batch Processing Function - Azure Functions runtime

### Path Conventions

**All paths in this guide are relative to the repository root directory:**

```bash
chat-with-your-data-solution-accelerator/    ‚Üê Repository root (start here)
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ create_app.py
‚îÇ   ‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Admin.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_push_results.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_start_processing.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ function_app.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local.settings.json.sample  ‚Üê Function config sample
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .env                        ‚Üê Batch function config file
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ .azure/
‚îÇ   ‚îî‚îÄ‚îÄ <env-name>/
‚îÇ       ‚îî‚îÄ‚îÄ .env                            ‚Üê Generated by azd provision
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îú‚îÄ‚îÄ main.bicep
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îî‚îÄ‚îÄ docs/                                   ‚Üê Documentation (you are here)
```

**Before starting any step, ensure you are in the repository root directory:**

```bash
# Verify you're in the correct location
pwd  # Linux/macOS - should show: .../chat-with-your-data-solution-accelerator
Get-Location  # Windows PowerShell - should show: ...\chat-with-your-data-solution-accelerator

# If not, navigate to repository root
cd path/to/chat-with-your-data-solution-accelerator
```

### Configuration Files

This project uses environment variables stored in `.env` files:

- **Batch Function**: `code/backend/batch/.env` - Copy from `.azure/<env-name>/.env` after provisioning
- **Frontend**: `code/frontend/` - Update `vite.config.ts` with API URL
- **Admin**: Uses environment variables from the parent shell or VS Code launch configuration

> **‚ö†Ô∏è Important**: Most environment variables are automatically generated when you run `azd provision`. You'll need to manually configure some values as described in the steps below.

## Step 1: Prerequisites - Install Required Tools

### Windows Development

#### Option 1: Native Windows (PowerShell)

```powershell
# Install Python 3.11
winget install Python.Python.3.11

# Install Node.js LTS
winget install OpenJS.NodeJS.LTS

# Install Azure Developer CLI
winget install Microsoft.AzureCLI

# Install Azure Functions Core Tools
winget install Microsoft.Azure.FunctionsCoreTools

# Install Visual Studio Code
winget install Microsoft.VisualStudioCode
```

#### Option 2: Windows with WSL2 (Recommended)

```powershell
# Install WSL2 first (run in PowerShell as Administrator)
wsl --install -d Ubuntu

# Then in WSL2 Ubuntu terminal:
sudo apt update && sudo apt install python3.11 python3.11-venv git curl nodejs npm -y

# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Install Azure Functions Core Tools
wget -q https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
sudo apt-get update
sudo apt-get install azure-functions-core-tools-4

# Install Azure Developer CLI
curl -fsSL https://aka.ms/install-azd.sh | bash
```

### Linux Development

#### Ubuntu/Debian

```bash
# Install prerequisites
sudo apt update && sudo apt install python3.11 python3.11-venv git curl nodejs npm -y

# Install Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Install Azure Functions Core Tools
wget -q https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
sudo apt-get update
sudo apt-get install azure-functions-core-tools-4

# Install Azure Developer CLI
curl -fsSL https://aka.ms/install-azd.sh | bash
```

#### RHEL/CentOS/Fedora

```bash
# Install prerequisites
sudo dnf install python3.11 python3.11-devel git curl gcc nodejs npm -y

# Install Azure CLI
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc
sudo dnf install azure-cli

# Install Azure Functions Core Tools
sudo dnf install azure-functions-core-tools-4

# Install Azure Developer CLI
curl -fsSL https://aka.ms/install-azd.sh | bash
```

### macOS Development

```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install prerequisites
brew install python@3.11 node azure-cli azure-functions-core-tools@4

# Install Azure Developer CLI
curl -fsSL https://aka.ms/install-azd.sh | bash
```

## Step 2: Development Tools Setup

### Visual Studio Code (Recommended)

#### Required Extensions

Create `.vscode/extensions.json` in the workspace root and copy the following JSON:

```json
{
    "recommendations": [
        "ms-python.python",
        "ms-python.pylint",
        "ms-python.black-formatter",
        "ms-azuretools.vscode-azurefunctions",
        "ms-azuretools.vscode-bicep",
        "ms-python.vscode-pylance",
        "ms-vscode.azure-account",
        "ms-vscode-remote.remote-wsl",
        "ms-teams-vscode.teams-toolkit"
    ]
}
```

VS Code will prompt you to install these recommended extensions when you open the workspace.

#### Settings Configuration

Create `.vscode/settings.json` and copy the following JSON:

```json
{
    "python.defaultInterpreterPath": "./.venv/bin/python",
    "python.terminal.activateEnvironment": true,
    "python.linting.enabled": true,
    "python.linting.pylintEnabled": true,
    "python.testing.pytestEnabled": true,
    "python.testing.unittestEnabled": false,
    "files.associations": {
        "*.bicep": "bicep"
    }
}
```

## Step 3: Clone the Repository

```bash
git clone https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator.git
cd chat-with-your-data-solution-accelerator
```
## Step 4: Azure Authentication and Resource Provisioning

Before running the application locally, you need to provision Azure resources and configure authentication.

### 4.1. Azure CLI Login

```bash
# Login to Azure CLI
az login

# Set your subscription
az account set --subscription "your-subscription-id"

# Verify authentication
az account show
```

### 4.2. Provision Azure Resources

The application requires Azure resources to be provisioned before running locally. Use Azure Developer CLI (azd) to provision all resources:

```bash
# Initialize azd environment (first time only)
azd init

# Provision all Azure resources
azd provision
```

This command will:
- Create all required Azure resources (Storage, CosmosDB/PostgreSQL, AI Search, OpenAI, etc.)
- Generate a `.env` file in `.azure/<env-name>/.env` with all configuration values
- Set up the infrastructure using Bicep templates

> **‚ö†Ô∏è Important**: The `azd provision` command can take 15-30 minutes to complete. Do not interrupt this process.

### 4.3. Required Azure RBAC Permissions

To run the application locally using RBAC authentication (recommended), your Azure account needs the following role assignments:

#### Option 1: Assign Roles Manually

Get your Principal ID from Microsoft Entra ID, then assign these roles:

| Role | GUID | Scope |
|----|----|----|
| Cognitive Services OpenAI User | 5e0bd9bd-7b93-4f28-af87-19fc36ad61bd | Azure OpenAI Service |
| Cognitive Services User | a97b65f3-24c7-4388-baec-2e87135dc908 | Cognitive Services |
| Cosmos DB Built-in Data Contributor | 00000000-0000-0000-0000-000000000002 | Cosmos DB Account ([How to assign](https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-setup-rbac#role-assignments)) |
| Key Vault Secrets User | 4633458b-17de-408a-b874-0445c86b69e6 | Key Vault |
| Search Index Data Contributor | 8ebe5a00-799e-43f5-93ac-243d3dce84a7 | AI Search Service |
| Search Service Contributor | 7ca78c08-252a-4471-8644-bb5ff32d4ba0 | AI Search Service |
| Storage Blob Data Contributor | ba92f5b4-2d11-453d-a403-e96b0029c9fe | Storage Account |
| Storage Queue Data Contributor | 974c5e8b-45b9-4653-ba55-5f855dd0fb88 | Storage Account |

#### Option 2: Assign Roles Programmatically

See [Step 13: Assign Azure Roles Using Command Line](#step-13-assign-azure-roles-using-command-line) for automated role assignment scripts.

#### Option 3: Update Bicep Template

You can also update the `principalId` value in `infra/main.bicep` with your Principal ID before running `azd provision`. This will automatically assign roles during provisioning.

**Note**: RBAC permission changes can take 5-10 minutes to propagate. If you encounter "Forbidden" errors after assigning roles, wait a few minutes and try again.

### 4.4. Configure Authentication Type

The application supports two authentication methods:

#### RBAC Authentication (Recommended)

1. Ensure role assignments from Step 4.3 are created
2. Navigate to your Search service in the Azure Portal
3. Under **Settings**, select **Keys**
4. Select either **Role-based access control** or **Both**
5. Set `AZURE_AUTH_TYPE=rbac` in your `.env` file
6. Set `APP_ENV=dev` in your `.env` file to use Azure CLI credentials locally

#### API Key Authentication

1. Set `AZURE_AUTH_TYPE=keys` in your `.env` file
2. Ensure all necessary API keys are present in the `.env` file

> **‚ö†Ô∏è Security Note**: RBAC authentication is more secure and recommended for production deployments. API keys should only be used for development and testing.

## Step 5: Set Up Python Virtual Environment

> **üìã Note**: Python 3.11 is required for this project. Ensure it's installed before proceeding.

### Option 1: Windows (PowerShell)

```powershell
# Create virtual environment
python -m venv .venv

# Activate virtual environment
.\.venv\Scripts\Activate.ps1
```

### Option 2: Linux/macOS/WSL2

```bash
# Create virtual environment
python3.11 -m venv .venv

# Activate virtual environment
source .venv/bin/activate
```

## Step 6: Install Python Dependencies

Install dependencies for all Python services:

### 6.1. Install Backend Dependencies

```bash
# Navigate to backend directory
cd code/backend

# Install dependencies using Poetry
pip install --upgrade pip
pip install poetry
poetry export -o requirements.txt
pip install -r requirements.txt
```

### 6.2. Install Batch Function Dependencies

```bash
# Navigate to batch directory
cd ../backend/batch

# Install dependencies
pip install -r requirements.txt
```

### 6.3. Alternative: Use Setup Script

```bash
# From repository root
.devcontainer/setupEnv.sh
```

This script will install all dependencies for backend, batch, and utilities folders automatically.

## Step 7: Frontend (UI) Setup & Run Instructions

> **üìã Terminal Reminder**: Open a **dedicated terminal window (Terminal 1)** for the Frontend service. All commands in this section assume you start from the **repository root directory**.

The UI is a React-based application located under `code/frontend`.

### 7.1. Navigate to Frontend Directory

```bash
# From repository root
cd code/frontend
```

### 7.2. Install Frontend Dependencies

```bash
npm install
```

### 7.3. Configure Frontend API URL

Update `vite.config.ts` to point to the Flask API URL. For local development, the default Flask API runs at `http://127.0.0.1:5050`.

Edit [vite.config.ts](../code/frontend/vite.config.ts):

```typescript
export default defineConfig({
  // ... other config
  server: {
    proxy: {
      '/api': {
        target: 'http://127.0.0.1:5050',  // Update this to your API URL
        changeOrigin: true,
      }
    }
  }
})
```

### 7.4. Run the Frontend

#### Option A: Using VS Code (Recommended)

1. Press `Ctrl+Shift+D` to open the Run and Debug panel
2. Select **"Launch Frontend (UI)"** from the dropdown
3. Click the green play button or press `F5`

This will:
- Automatically install dependencies
- Start the Vite development server
- Enable hot reload for instant updates
- Allow setting breakpoints for debugging

#### Option B: Command Line

```bash
# Make sure you're in code/frontend directory
npm run dev
```

The frontend will start at:

```
http://localhost:5174
```

(or whichever port Vite assigns - check the terminal output)

> **‚ö†Ô∏è Hot Reload**: Changes to React components will automatically reload in the browser. No need to manually refresh!

## Step 8: Backend API Setup & Run Instructions

> **üìã Terminal Reminder**: Open a **second dedicated terminal window (Terminal 2)** for the Backend API (Flask). Keep Terminal 1 (Frontend) running. All commands assume you start from the **repository root directory**.

The Backend API is a Python Flask application that provides REST endpoints for the frontend.

### 8.1. Navigate to Code Directory

```bash
# From repository root
cd code
```

### 8.2. Configure Backend Environment

Ensure your virtual environment is activated and the `.env` file from `azd provision` is accessible. The Flask app will read environment variables from `.azure/<env-name>/.env`.

### 8.3. Run the Backend API

#### Option A: Using VS Code (Recommended)

1. Press `Ctrl+Shift+D` to open the Run and Debug panel
2. Select **"Launch Frontend (api)"** from the dropdown
3. Click the green play button or press `F5`

This will:
- Automatically activate the virtual environment
- Start the Flask development server
- Enable hot reload for code changes
- Allow setting breakpoints for debugging

#### Option B: Command Line

```bash
# Make sure you're in code directory
# Ensure virtual environment is activated
poetry run flask run
```

The Flask API will start at:

```
http://127.0.0.1:5050
```

> **‚ö†Ô∏è Important**: Make sure this port matches the proxy configuration in `vite.config.ts` (Step 7.3)

**API Endpoints:**
- Health check: `http://127.0.0.1:5050/health`
- API routes: `http://127.0.0.1:5050/api/...`

## Step 9: Admin Web Setup & Run Instructions

> **üìã Terminal Reminder**: Open a **third dedicated terminal window (Terminal 3)** for the Admin Web (Streamlit). Keep Terminals 1 (Frontend) and 2 (Backend API) running. All commands assume you start from the **repository root directory**.

The Admin application is a Streamlit-based interface for managing documents, viewing data, and configuring the solution.

### 9.1. Navigate to Backend Directory

```bash
# From repository root
cd code/backend
```

### 9.2. Run the Admin Application

#### Option A: Using VS Code (Recommended)

1. Press `Ctrl+Shift+D` to open the Run and Debug panel
2. Select **"Launch Admin site"** from the dropdown
3. Click the green play button or press `F5`

This will:
- Automatically start the Streamlit server
- Open the browser at `http://localhost:8501`
- Enable hot reload for code changes
- Allow setting breakpoints for debugging

#### Option B: Command Line

```bash
# Make sure you're in code/backend directory
# Ensure virtual environment is activated
streamlit run Admin.py
```

The Admin interface will automatically open in your browser at:

```
http://localhost:8501
```

**Admin Features:**
- Upload and manage documents
- Configure search indexes
- View conversation logs
- Test embeddings and queries
- Monitor system health

## Step 10: Batch Processing Function Setup & Run Instructions

> **üìã Terminal Reminder**: The Batch Processing Function runs in the background. You can run it in Terminal 3 if you don't need the Admin Web, or open a **fourth terminal window (Terminal 4)**.

The Batch Processing Function is an Azure Function that handles document processing, chunking, and indexing.

### 10.1. Configure Batch Function Environment

Navigate to the batch function directory and configure the local settings:

```bash
# From repository root
cd code/backend/batch
```

#### Create local.settings.json

```bash
# Copy the sample file
cp local.settings.json.sample local.settings.json  # Linux/macOS
# or
Copy-Item local.settings.json.sample local.settings.json  # Windows PowerShell
```

Edit `local.settings.json` and update the following:

```json
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage__accountName": "<your-storage-account-name>",
    "FUNCTIONS_WORKER_RUNTIME": "python"
  }
}
```

#### Copy .env File

The batch function needs access to the same environment variables as other services:

```bash
# Copy .env from the azd-generated location
cp ../../../.azure/<env-name>/.env .  # Linux/macOS
# or
Copy-Item ..\..\..\..azure\<env-name>\.env .  # Windows PowerShell
```

> **‚ö†Ô∏è Important**: Replace `<env-name>` with your actual azd environment name. You can find it by running `azd env list`.

### 10.2. Install Batch Function Dependencies

```bash
# Make sure you're in code/backend/batch directory
pip install -r requirements.txt
```

### 10.3. Run the Batch Function

#### Option A: Using Azure Functions Core Tools (Command Line)

```bash
# Make sure you're in code/backend/batch directory
# Ensure virtual environment is activated
poetry run func start
```

#### Option B: Using VS Code Azure Functions Extension

1. Install the [Azure Functions VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions)
2. Press `F5` with the batch function folder open
3. Or right-click on the `function_app.py` file and select **"Execute Function Now..."**

The Azure Functions runtime will start and display available HTTP trigger URLs:

```
Functions:
    batch_push_results: [POST] http://localhost:7071/api/batch_push_results
    batch_start_processing: [POST] http://localhost:7071/api/batch_start_processing
```

**Triggering Functions:**
- Use the URLs displayed in the terminal
- Use tools like Postman or curl to send HTTP requests
- Upload documents through the Admin interface (which will trigger the function)

> **‚ö†Ô∏è Important**: You may need to stop the deployed function in the Azure Portal to ensure all requests are processed locally during debugging.

## Step 11: Verify All Services Are Running

Before using the application, confirm all services are running correctly:

### Terminal Status Checklist

| Terminal | Service | Command | Expected Output | URL |
|----------|---------|---------|-----------------|-----|
| **Terminal 1** | Frontend (React/Vite) | `npm run dev` | `Local: http://localhost:5174/` | http://localhost:5174 |
| **Terminal 2** | Backend API (Flask) | `poetry run flask run` | `Running on http://127.0.0.1:5050` | http://127.0.0.1:5050 |
| **Terminal 3** | Admin Web (Streamlit) | `streamlit run Admin.py` | `Local URL: http://localhost:8501` | http://localhost:8501 |
| **Terminal 4** (Optional) | Batch Function | `poetry run func start` | `Functions: batch_push_results: [POST] http://localhost:7071/api/...` | http://localhost:7071 |

### Quick Verification

**1. Check Backend API:**
```bash
# In a new terminal
curl http://127.0.0.1:5050/health
# Expected: JSON response with health status
```

**2. Check Frontend:**
- Open browser to http://localhost:5174
- Should see the Chat interface
- Try typing a message (requires backend to be running)

**3. Check Admin Interface:**
- Open browser to http://localhost:8501
- Should see the Admin dashboard
- Navigate through different pages to verify functionality

**4. Check Batch Function (if running):**
```bash
# In a new terminal
curl http://localhost:7071/api/batch_push_results -X POST
# Should return a response (may be an error if no data, but confirms it's running)
```

### Common Issues

**Service not starting?**
- Ensure you're in the correct directory
- Verify virtual environment is activated (Python services)
- Check that the port is not already in use
- Review error messages in the terminal
- Ensure `azd provision` completed successfully

**Can't access services?**
- Verify firewall isn't blocking the ports
- Try `http://localhost:port` instead of `http://127.0.0.1:port` (or vice versa)
- Ensure services show "startup complete" messages
- Check VS Code terminal output for errors

**Environment variable errors?**
- Verify `.azure/<env-name>/.env` file exists and contains values
- Check that `APP_ENV=dev` is set for local development
- Ensure Azure CLI is logged in (`az account show`)
- Verify RBAC roles have been assigned and propagated

## Step 12: Alternative Deployment Options

### Deploy Services Manually Using azd

You can deploy the full solution or individual services using Azure Developer CLI:

```bash
# Deploy all services
azd deploy

# Deploy individual services
azd deploy web       # Frontend chat application
azd deploy adminweb  # Admin Streamlit application
azd deploy function  # Batch processing function
```

### Running All Services Using Docker Compose

For a containerized local development environment:

#### Prerequisites

1. Ensure Docker Desktop is installed and running
2. Provision Azure resources using `azd provision` to generate `.env` file

#### Setup

1. **Locate the .env file:**
   ```bash
   # Find your environment name
   azd env list

   # The .env file will be at:
   # .azure/<env-name>/.env
   ```

2. **Configure APP_ENV for local development:**

   Edit `.azure/<env-name>/.env` and set:
   ```bash
   APP_ENV=dev  # Use Azure CLI credentials for local development
   ```

   For production, set `APP_ENV=prod` to use Managed Identity.

3. **Add AzureWebJobsStorage (required):**

   This value needs to be added manually. Get it from the Azure Portal:
   - Navigate to your Function App
   - Go to **Settings** ‚Üí **Configuration**
   - Copy the `AzureWebJobsStorage` value
   - Add it to your `.env` file

#### Run with Docker Compose

```bash
# Option 1: Using Make
make docker-compose-up

# Option 2: Using docker-compose directly
cd docker
AZD_ENV_FILE=../.azure/<env-name>/.env docker-compose up

# Windows PowerShell:
$env:AZD_ENV_FILE="../.azure/<env-name>/.env"
docker-compose up
```

**Note:** By default, these commands will run the latest Docker images built from the main branch. To use custom images:
1. Build your own images using the Dockerfiles in the `docker/` directory
2. Update `docker/docker-compose.yml` with your image tags
3. Run `docker-compose up`

## Step 13: Assign Azure Roles Using Command Line

For automated role assignment, use the following script. Replace the placeholder values with your actual Azure resource information:

```bash
#!/bin/bash

# Variables - Update these with your values
SUBSCRIPTION_ID="<your-subscription-id>"
RESOURCE_GROUP="<your-resource-group>"
PRINCIPAL_ID="<user-or-service-principal-id>"
SOLUTION_PREFIX="<your-solution-prefix>"

# Get your principal ID automatically (if using current user)
PRINCIPAL_ID=$(az ad signed-in-user show --query id -o tsv)

# Azure AI Search
az role assignment create --assignee $PRINCIPAL_ID --role "Search Index Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Search/searchServices/srch-$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Search Service Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Search/searchServices/srch-$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Search Index Data Reader" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Search/searchServices/srch-$SOLUTION_PREFIX

# Azure OpenAI
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/oai-$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services OpenAI User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/oai-$SOLUTION_PREFIX

# Computer Vision (if enabled)
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/cv-$SOLUTION_PREFIX

# Speech Services (if enabled)
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/spch-$SOLUTION_PREFIX

# Document Intelligence
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/di-$SOLUTION_PREFIX

# Content Safety (if enabled)
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/cs-$SOLUTION_PREFIX

# Storage Account
az role assignment create --assignee $PRINCIPAL_ID --role "Storage Blob Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/st$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Storage Queue Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/st$SOLUTION_PREFIX

# Key Vault
az role assignment create --assignee $PRINCIPAL_ID --role "Key Vault Secrets User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.KeyVault/vaults/kv-$SOLUTION_PREFIX

# Cosmos DB (if using CosmosDB)
az cosmosdb sql role assignment create --account-name cosmos-$SOLUTION_PREFIX --resource-group $RESOURCE_GROUP --scope "/" --principal-id $PRINCIPAL_ID --role-definition-id 00000000-0000-0000-0000-000000000002

# PostgreSQL (if using PostgreSQL) - Add user as administrator
# az postgres flexible-server ad-admin create --server-name <server-name> --resource-group $RESOURCE_GROUP --object-id $PRINCIPAL_ID --display-name <display-name>
```

**Windows PowerShell Version:**

```powershell
# Variables - Update these with your values
$SUBSCRIPTION_ID = "<your-subscription-id>"
$RESOURCE_GROUP = "<your-resource-group>"
$SOLUTION_PREFIX = "<your-solution-prefix>"

# Get your principal ID automatically
$PRINCIPAL_ID = (az ad signed-in-user show --query id -o tsv)

# Azure AI Search
az role assignment create --assignee $PRINCIPAL_ID --role "Search Index Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Search/searchServices/srch-$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Search Service Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Search/searchServices/srch-$SOLUTION_PREFIX

# Azure OpenAI
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/oai-$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Cognitive Services OpenAI User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.CognitiveServices/accounts/oai-$SOLUTION_PREFIX

# Storage Account
az role assignment create --assignee $PRINCIPAL_ID --role "Storage Blob Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/st$SOLUTION_PREFIX
az role assignment create --assignee $PRINCIPAL_ID --role "Storage Queue Data Contributor" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/st$SOLUTION_PREFIX

# Key Vault
az role assignment create --assignee $PRINCIPAL_ID --role "Key Vault Secrets User" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.KeyVault/vaults/kv-$SOLUTION_PREFIX

# Cosmos DB (if using CosmosDB)
az cosmosdb sql role assignment create --account-name cosmos-$SOLUTION_PREFIX --resource-group $RESOURCE_GROUP --scope "/" --principal-id $PRINCIPAL_ID --role-definition-id 00000000-0000-0000-0000-000000000002
```

> **‚ö†Ô∏è Important**:
> - Replace all placeholder values (`<your-subscription-id>`, `<your-resource-group>`, etc.) with your actual values
> - Some services (Computer Vision, Speech, Content Safety) are optional and may not exist in your deployment
> - RBAC changes can take 5-10 minutes to propagate
> - Run `azd env get-values` to see your solution prefix and other values

## Troubleshooting

### Common Issues

#### Python Version Issues

```bash
# Check available Python versions
python --version
python3.11 --version

# If python3.11 not found, install it:
# Ubuntu/Debian: sudo apt install python3.11 python3.11-venv
# Windows: winget install Python.Python.3.11
# macOS: brew install python@3.11
```

#### Virtual Environment Issues

```bash
# Recreate virtual environment
rm -rf .venv  # Linux/macOS
# or
Remove-Item -Recurse -Force .venv  # Windows PowerShell

# Create new virtual environment
python3.11 -m venv .venv  # Linux/macOS
python -m venv .venv  # Windows

# Activate and reinstall dependencies
source .venv/bin/activate  # Linux/macOS
# or
.\.venv\Scripts\Activate.ps1  # Windows

# Reinstall dependencies
cd code/backend
pip install --upgrade pip
pip install poetry
poetry export -o requirements.txt
pip install -r requirements.txt
```

#### Azure Authentication Issues

```bash
# Check Azure CLI authentication
az account show

# If not authenticated, login
az login

# Set correct subscription
az account set --subscription "your-subscription-id"

# Verify RBAC roles are assigned
az role assignment list --assignee $(az ad signed-in-user show --query id -o tsv)

# Force refresh tokens (if getting 401/403 errors)
az account get-access-token --resource https://management.azure.com/
```

#### Permission Issues (Linux/macOS)

```bash
# Fix ownership of files
sudo chown -R $USER:$USER .

# Fix script permissions
chmod +x .devcontainer/setupEnv.sh
chmod +x scripts/*.sh
```

#### Windows-Specific Issues

```powershell
# PowerShell execution policy
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Long path support (Windows 10 1607+, run as Administrator)
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" `
  -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force

# SSL certificate issues
pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org -r requirements.txt
```

#### Port Already in Use

```bash
# Find process using a port (Linux/macOS)
lsof -i :5050  # Backend Flask
lsof -i :5174  # Frontend Vite
lsof -i :8501  # Admin Streamlit
lsof -i :7071  # Azure Functions

# Kill process by PID
kill -9 <PID>

# Find process using a port (Windows)
netstat -ano | findstr :5050
netstat -ano | findstr :5174
netstat -ano | findstr :8501
netstat -ano | findstr :7071

# Kill process by PID (Windows)
taskkill /PID <PID> /F
```

#### Node.js/npm Issues

```bash
# Clear npm cache
npm cache clean --force

# Remove node_modules and reinstall
cd code/frontend
rm -rf node_modules package-lock.json  # Linux/macOS
# or
Remove-Item -Recurse -Force node_modules, package-lock.json  # Windows

npm install

# If npm install fails, try with legacy peer deps
npm install --legacy-peer-deps
```

#### Environment Variable Issues

```bash
# Check environment variables are loaded (Linux/macOS)
env | grep AZURE

# Check environment variables (Windows PowerShell)
Get-ChildItem Env:AZURE*

# Validate .env file format
cat .azure/<env-name>/.env | grep -v '^#' | grep '='  # Should show key=value pairs

# Check azd environment
azd env list
azd env get-values
```

#### Azure Functions Issues

```bash
# Ensure Azure Functions Core Tools is installed
func --version

# If not installed:
# Windows: winget install Microsoft.Azure.FunctionsCoreTools
# Linux/macOS: See Step 1 for installation instructions

# Check AzureWebJobsStorage configuration
cat code/backend/batch/local.settings.json

# Validate function app structure
ls code/backend/batch/
# Should see: function_app.py, local.settings.json, requirements.txt

# Clear Functions cache
rm -rf ~/.azure-functions-core-tools  # Linux/macOS
Remove-Item -Recurse ~/.azure-functions-core-tools  # Windows
```

#### Streamlit Issues

```bash
# Clear Streamlit cache
streamlit cache clear

# Run with verbose logging
streamlit run Admin.py --logger.level=debug

# Check Streamlit version
streamlit --version

# Reinstall Streamlit
pip uninstall streamlit
pip install streamlit
```

#### azd Provision Failures

```bash
# Check azd version
azd version

# Update azd to latest version
# Windows: winget upgrade Microsoft.Azd
# Linux/macOS: curl -fsSL https://aka.ms/install-azd.sh | bash

# Clean azd environment and retry
azd env delete <env-name>
azd init
azd provision

# Check Azure subscription quotas
# Many provision failures are due to quota limits
# Visit Azure Portal ‚Üí Quotas to check and request increases

# Enable detailed logging
azd provision --debug
```

#### Cosmos DB Connection Issues

```bash
# Verify Cosmos DB RBAC role assignment
az cosmosdb sql role assignment list \
  --account-name cosmos-<prefix> \
  --resource-group <resource-group>

# Check if Cosmos DB is accessible
az cosmosdb show \
  --name cosmos-<prefix> \
  --resource-group <resource-group>

# Test connection using Azure SDK
python -c "from azure.cosmos import CosmosClient; print('Cosmos DB SDK imported successfully')"
```

#### AI Search Issues

```bash
# Verify Search service is running
az search service show \
  --name srch-<prefix> \
  --resource-group <resource-group>

# Check RBAC roles for Search
az role assignment list \
  --scope /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Search/searchServices/srch-<prefix>

# Test search index
az search index list \
  --service-name srch-<prefix> \
  --resource-group <resource-group>
```

#### OpenAI Connection Issues

```bash
# Verify OpenAI service
az cognitiveservices account show \
  --name oai-<prefix> \
  --resource-group <resource-group>

# Check model deployments
az cognitiveservices account deployment list \
  --name oai-<prefix> \
  --resource-group <resource-group>

# Verify RBAC roles
az role assignment list \
  --scope /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.CognitiveServices/accounts/oai-<prefix>
```

#### Debugging Tips

1. **Enable Verbose Logging:**
   - Set `LOGLEVEL=DEBUG` in your `.env` file
   - Set `PACKAGE_LOGGING_LEVEL=DEBUG` for Azure SDK logging
   - Add `AZURE_LOGGING_PACKAGES=azure.core,azure.identity` to see authentication details

2. **Check Application Logs:**
   - Flask: Check the terminal running `flask run`
   - Streamlit: Check the terminal running `streamlit run`
   - Functions: Check the Functions Core Tools terminal output
   - Frontend: Check browser developer console (F12)

3. **Isolate the Issue:**
   - Test each service independently
   - Verify Azure resources are accessible from Azure Portal
   - Use Azure Storage Explorer to check blob/queue contents
   - Test API endpoints with curl or Postman

4. **Common Error Messages:**

   | Error | Likely Cause | Solution |
   |-------|--------------|----------|
   | `401 Unauthorized` | RBAC roles not assigned or not propagated | Wait 5-10 minutes, verify role assignments |
   | `403 Forbidden` | Insufficient permissions | Check RBAC roles, verify authentication |
   | `404 Not Found` | Resource doesn't exist or wrong name | Verify resource names in .env file |
   | `ModuleNotFoundError` | Missing Python dependency | Run `pip install -r requirements.txt` |
   | `EADDRINUSE` | Port already in use | Kill process using the port (see above) |
   | `azd provision failed` | Quota limits or infrastructure error | Check quotas, review error details |

## Step 14: Next Steps

Once all services are running successfully, you can:

1. **Access the Application**: Open `http://localhost:5174` in your browser to start chatting with your data
2. **Upload Documents**: Use the Admin interface at `http://localhost:8501` to upload and manage documents
3. **Test API Endpoints**: Use the Flask API at `http://127.0.0.1:5050/docs` to explore available endpoints
4. **Monitor Processing**: Watch the Batch Function logs to see document processing in action
5. **Customize the Solution**: Modify prompts, orchestration strategies, and UI components to fit your needs
6. **Deploy to Azure**: Use `azd deploy` to deploy your local changes to Azure

## Related Documentation

- [LOCAL_DEPLOYMENT.md](LOCAL_DEPLOYMENT.md) - Detailed local deployment scenarios and Docker options
- [best_practices.md](best_practices.md) - Best practices for production deployments
- [model_configuration.md](model_configuration.md) - Configure AI models and embeddings
- [conversation_flow_options.md](conversation_flow_options.md) - Customize conversation orchestration
- [integrated_vectorization.md](integrated_vectorization.md) - Set up integrated vectorization
- [azure_app_service_auth_setup.md](azure_app_service_auth_setup.md) - Configure authentication
- [web-apps.md](web-apps.md) - Deploy to Azure App Service
- [supported_file_types.md](supported_file_types.md) - Supported document formats
- [advanced_image_processing.md](advanced_image_processing.md) - Enable vision-based document processing

## Environment Variables

This section provides a comprehensive reference of all environment variables used by the Chat With Your Data Solution Accelerator. Most of these variables are automatically configured when you run `azd provision`, which generates them in the `.azure/<env-name>/.env` file.

**When to use this reference:**
- Customizing AI model behavior (temperature, tokens, prompts)
- Understanding what each setting controls
- Troubleshooting configuration issues
- Manually configuring advanced features
- Setting up custom orchestration strategies

> **üí° Tip**: After running `azd provision`, you can view all generated values with `azd env get-values`

### Complete Variable List

| App Setting | Value | Note |
| --- | --- | ------------- |
|ADVANCED_IMAGE_PROCESSING_MAX_IMAGES | 1 | The maximum number of images to pass to the vision model in a single request|
|APPLICATIONINSIGHTS_CONNECTION_STRING||The Application Insights connection string to store the application logs|
|APP_ENV | Prod | Application Environment (Prod, Dev, etc.)|
|AZURE_AUTH_TYPE | keys | The default is to use API keys. Change the value to 'rbac' to authenticate using Role Based Access Control. For more information refer to section [Authenticate using RBAC](#authenticate-using-rbac)|
|AZURE_BLOB_ACCOUNT_KEY||The key of the Azure Blob Storage for storing the original documents to be processed|
|AZURE_BLOB_ACCOUNT_NAME||The name of the Azure Blob Storage for storing the original documents to be processed|
|AZURE_BLOB_CONTAINER_NAME||The name of the Container in the Azure Blob Storage for storing the original documents to be processed|
|AZURE_CLIENT_ID | | Client ID for Azure authentication (required for LangChain AzureSearch vector store)|
|AZURE_COMPUTER_VISION_ENDPOINT | | The endpoint of the Azure Computer Vision service (if useAdvancedImageProcessing=true)|
|AZURE_COMPUTER_VISION_VECTORIZE_IMAGE_API_VERSION | 2024-02-01 | The API version for Azure Computer Vision Vectorize Image|
|AZURE_COMPUTER_VISION_VECTORIZE_IMAGE_MODEL_VERSION | 2023-04-15 | The model version for Azure Computer Vision Vectorize Image|
|AZURE_CONTENT_SAFETY_ENDPOINT | | The endpoint of the Azure AI Content Safety service|
|AZURE_CONTENT_SAFETY_KEY | | The key of the Azure AI Content Safety service|
|AZURE_COSMOSDB_ACCOUNT_NAME | | The name of the Azure Cosmos DB account (when using CosmosDB)|
|AZURE_COSMOSDB_CONVERSATIONS_CONTAINER_NAME | | The name of the Azure Cosmos DB conversations container (when using CosmosDB)|
|AZURE_COSMOSDB_DATABASE_NAME | | The name of the Azure Cosmos DB database (when using CosmosDB)|
|AZURE_COSMOSDB_ENABLE_FEEDBACK | true | Whether to enable feedback functionality in Cosmos DB|
|AZURE_FORM_RECOGNIZER_ENDPOINT||The name of the Azure Form Recognizer for extracting the text from the documents|
|AZURE_FORM_RECOGNIZER_KEY||The key of the Azure Form Recognizer for extracting the text from the documents|
|AZURE_KEY_VAULT_ENDPOINT | | The endpoint of the Azure Key Vault for storing secrets|
|AZURE_OPENAI_API_KEY||One of the API keys of your Azure OpenAI resource|
|AZURE_OPENAI_API_VERSION|2024-02-01|API version when using Azure OpenAI on your data|
|AZURE_OPENAI_EMBEDDING_MODEL|text-embedding-ada-002|The name of your Azure OpenAI embeddings model deployment|
|AZURE_OPENAI_EMBEDDING_MODEL_NAME|text-embedding-ada-002|The name of the embeddings model (can be found in Azure AI Foundry)|
|AZURE_OPENAI_EMBEDDING_MODEL_VERSION|2|The version of the embeddings model to use (can be found in Azure AI Foundry)|
|AZURE_OPENAI_MAX_TOKENS|1000|The maximum number of tokens allowed for the generated answer.|
|AZURE_OPENAI_MODEL||The name of your model deployment|
|AZURE_OPENAI_MODEL_NAME|gpt-4.1|The name of the model|
|AZURE_OPENAI_MODEL_VERSION|2024-05-13|The version of the model to use|
|AZURE_OPENAI_RESOURCE||the name of your Azure OpenAI resource|
|AZURE_OPENAI_STOP_SEQUENCE||Up to 4 sequences where the API will stop generating further tokens. Represent these as a string joined with "|", e.g. `"stop1|stop2|stop3"`|
|AZURE_OPENAI_STREAM | true | Whether or not to stream responses from Azure OpenAI|
|AZURE_OPENAI_SYSTEM_MESSAGE|You are an AI assistant that helps people find information.|A brief description of the role and tone the model should use|
|AZURE_OPENAI_TEMPERATURE|0|What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. A value of 0 is recommended when using your data.|
|AZURE_OPENAI_TOP_P|1.0|An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. We recommend setting this to 1.0 when using your data.|
|AZURE_POSTGRESQL_DATABASE_NAME | postgres | The name of the Azure PostgreSQL database (when using PostgreSQL)|
|AZURE_POSTGRESQL_HOST_NAME | | The hostname of the Azure PostgreSQL server (when using PostgreSQL)|
|AZURE_POSTGRESQL_USER | | The username for Azure PostgreSQL authentication (when using PostgreSQL)|
|AZURE_SEARCH_CHUNK_COLUMN | chunk | Field from your Azure AI Search index that contains chunk information|
|AZURE_SEARCH_CONTENT_COLUMN||List of fields in your Azure AI Search index that contains the text content of your documents to use when formulating a bot response. Represent these as a string joined with "|", e.g. `"product_description|product_manual"`|
|AZURE_SEARCH_CONTENT_VECTOR_COLUMN||Field from your Azure AI Search index for storing the content's Vector embeddings|
|AZURE_SEARCH_CONVERSATIONS_LOG_INDEX | conversations | The name of the Azure AI Search conversations log index|
|AZURE_SEARCH_DATASOURCE_NAME | | The name of the Azure AI Search datasource|
|AZURE_SEARCH_DIMENSIONS|1536| Azure OpenAI Embeddings dimensions. 1536 for `text-embedding-ada-002`. A full list of dimensions can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#embeddings-models). |
|AZURE_SEARCH_ENABLE_IN_DOMAIN|True|Limits responses to only queries relating to your data.|
|AZURE_SEARCH_FIELDS_ID|id|`AZURE_SEARCH_FIELDS_ID`: Field from your Azure AI Search index that gives a unique identifier of the document chunk. `id` if you don't have a specific requirement.|
|AZURE_SEARCH_FIELDS_METADATA|metadata|Field from your Azure AI Search index that contains metadata for the document. `metadata` if you don't have a specific requirement.|
|AZURE_SEARCH_FIELDS_TAG|tag|Field from your Azure AI Search index that contains tags for the document. `tag` if you don't have a specific requirement.|
|AZURE_SEARCH_FILENAME_COLUMN||`AZURE_SEARCH_FILENAME_COLUMN`: Field from your Azure AI Search index that gives a unique identifier of the source of your data to display in the UI.|
|AZURE_SEARCH_FILTER||Filter to apply to search queries.|
|AZURE_SEARCH_INDEX||The name of your Azure AI Search Index|
|AZURE_SEARCH_INDEXER_NAME | | The name of the Azure AI Search indexer|
|AZURE_SEARCH_INDEX_IS_PRECHUNKED | false | Whether the search index is prechunked|
|AZURE_SEARCH_KEY||An **admin key** for your Azure AI Search resource|
|AZURE_SEARCH_LAYOUT_TEXT_COLUMN|layoutText|Field from your Azure AI Search index that contains the layout-aware text content of your documents. `layoutText` if you don't have a specific requirement.|
|AZURE_SEARCH_OFFSET_COLUMN | offset | Field from your Azure AI Search index that contains offset information|
|AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG|default|The name of the semantic search configuration to use if using semantic search.|
|AZURE_SEARCH_SERVICE||The URL of your Azure AI Search resource. e.g. https://<search-service>.search.windows.net|
|AZURE_SEARCH_SOURCE_COLUMN|source|Field from your Azure AI Search index that identifies the source of your data. `source` if you don't have a specific requirement.|
|AZURE_SEARCH_TEXT_COLUMN|text|Field from your Azure AI Search index that contains the main text content of your documents. `text` if you don't have a specific requirement.|
|AZURE_SEARCH_TITLE_COLUMN||Field from your Azure AI Search index that gives a relevant title or header for your data content to display in the UI.|
|AZURE_SEARCH_TOP_K|5|The number of documents to retrieve from Azure AI Search.|
|AZURE_SEARCH_URL_COLUMN||Field from your Azure AI Search index that contains a URL for the document, e.g. an Azure Blob Storage URI. This value is not currently used.|
|AZURE_SEARCH_USE_INTEGRATED_VECTORIZATION ||Whether to use [Integrated Vectorization](https://learn.microsoft.com/en-us/azure/search/vector-search-integrated-vectorization). If the database type is PostgreSQL, set this to false.|
|AZURE_SEARCH_USE_SEMANTIC_SEARCH|False|Whether or not to use semantic search|
|AZURE_SPEECH_RECOGNIZER_LANGUAGES | en-US,fr-FR,de-DE,it-IT | Comma-separated list of languages to recognize from speech input|
|AZURE_SPEECH_REGION_ENDPOINT | | The regional endpoint of the Azure Speech service|
|AZURE_SPEECH_SERVICE_KEY | | The key of the Azure Speech service|
|AZURE_SPEECH_SERVICE_NAME | | The name of the Azure Speech service|
|AZURE_SPEECH_SERVICE_REGION | | The region (location) of the Azure Speech service|
|AzureWebJobsStorage__accountName||The name of the Azure Blob Storage account for the Azure Functions Batch processing|
|BACKEND_URL||The URL for the Backend Batch Azure Function. Use http://localhost:7071 for local execution|
|CONVERSATION_FLOW | custom | Chat conversation type: custom or byod (Bring Your Own Data)|
|DATABASE_TYPE | PostgreSQL | The type of database to deploy (cosmos or postgres)|
|DOCUMENT_PROCESSING_QUEUE_NAME|doc-processing|The name of the Azure Queue to handle the Batch processing|
|FUNCTION_KEY | | The function key for accessing the backend Azure Function|
|LOGLEVEL | INFO | The log level for application logging (CRITICAL, ERROR, WARN, INFO, DEBUG)|
|PACKAGE_LOGGING_LEVEL | WARNING | Enhanced: Azure SDK package logging level (CRITICAL, ERROR, WARN, INFO, DEBUG)|
|AZURE_LOGGING_PACKAGES | (optional) | Enhanced: Comma-separated list of Azure logger packages to configure. If not provided, no Azure SDK logging is configured.|
|MANAGED_IDENTITY_CLIENT_ID | | The client ID of the user-assigned managed identity|
|MANAGED_IDENTITY_RESOURCE_ID | | The resource ID of the user-assigned managed identity|
|OPEN_AI_FUNCTIONS_SYSTEM_PROMPT | | System prompt for OpenAI functions orchestration|
|ORCHESTRATION_STRATEGY | openai_function | Orchestration strategy. Use Azure OpenAI Functions (openai_function), Semantic Kernel (semantic_kernel),  LangChain (langchain) or Prompt Flow (prompt_flow) for messages orchestration. If you are using a new model version 0613 select any strategy, if you are using a 0314 model version select "langchain". Note that both `openai_function` and `semantic_kernel` use OpenAI function calling. Prompt Flow option is still in development and does not support RBAC or integrated vectorization as of yet.|
|SEMANTIC_KERNEL_SYSTEM_PROMPT | | System prompt used by the Semantic Kernel orchestration|
|USE_ADVANCED_IMAGE_PROCESSING | false | Whether to enable the use of a vision LLM and Computer Vision for embedding images. If the database type is PostgreSQL, set this to false.|
|USE_KEY_VAULT | true | Whether to use Azure Key Vault for storing secrets|
